{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.jpg\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"5\"><b>Exploring SAR Data and SAR Time Series Analysis using Jupyter Notebooks</b></font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"><b> Franz J Meyer; University of Alaska Fairbanks & Josef Kellndorfer, <a href=\"http://earthbigdata.com/\" target=\"_blank\">Earth Big Data, LLC</a> </b> <br>\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" />\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This notebook will introduce you to the analysis of deep multi-temporal SAR image data stacks in the framework of *Jupyter Notebooks*. The Jupyter Notebook environment is easy to launch in any web browser for interactive data exploration with provided or new training data. Notebooks are comprised of text written in a combination of executable python code and markdown formatting including latex style mathematical equations. Another advantage of Jupyter Notebooks is that they can easily be expanded, changed, and shared with new data sets or newly available time series steps. Therefore, they provide an excellent basis for collaborative and repeatable data analysis. <br>\n",
    "\n",
    "<b>We introduce the following data analysis concepts:</b>\n",
    "\n",
    "- How to load your own SAR data into Jupyter Notebooks and create a time series stack \n",
    "- How to apply calibration constants to covert initial digital number (DN) data into calibrated radar cross section information.\n",
    "- How to subset images and create a time series of your subset data.\n",
    "- How to explore the time-series information in SAR data stacks for environmental analysis.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"5\" color=\"red\"> <b>Important Note about JupyterHub</b> </font>\n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"> <b>Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook.</b> </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 0. Importing Relevant Python Packages </b> </font>\n",
    "\n",
    "<font size=\"3\">In this notebook we will use the following scientific libraries:\n",
    "<ol type=\"1\">\n",
    "    <li> <b><a href=\"https://pandas.pydata.org/\" target=\"_blank\">Pandas</a></b> is a Python library that provides high-level data structures and a vast variety of tools for analysis. The great feature of this package is the ability to translate rather complex operations with data into one or two commands. Pandas contains many built-in methods for filtering and combining data, as well as the time-series functionality. </li>\n",
    "    <li> <b><a href=\"https://www.gdal.org/\" target=\"_blank\">GDAL</a></b> is a software library for reading and writing raster and vector geospatial data formats. It includes a collection of programs tailored for geospatial data processing. Most modern GIS systems (such as ArcGIS or QGIS) use GDAL in the background.</li>\n",
    "    <li> <b><a href=\"http://www.numpy.org/\" target=\"_blank\">NumPy</a></b> is one of the principal packages for scientific applications of Python. It is intended for processing large multidimensional arrays and matrices, and an extensive collection of high-level mathematical functions and implemented methods makes it possible to perform various operations with these objects. </li>\n",
    "    <li> <b><a href=\"https://matplotlib.org/index.html\" target=\"_blank\">Matplotlib</a></b> is a low-level library for creating two-dimensional diagrams and graphs. With its help, you can build diverse charts, from histograms and scatterplots to non-Cartesian coordinates graphs. Moreover, many popular plotting libraries are designed to work in conjunction with matplotlib. </li>\n",
    "    <li> The <b><a href=\"https://www.pydoc.io/pypi/asf-hyp3-1.1.1/index.html\" target=\"_blank\">asf-hyp3 API</a></b> provides useful functions and scripts for accessing and processing SAR data via the Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline, or HyP3 (pronounced \"hype\"). </li>\n",
    "<li><b><a href=\"https://www.scipy.org/about.html\" target=\"_blank\">SciPY</a></b> is a library that provides functions for numerical integration, interpolation, optimization, linear algebra and statistics. </li>\n",
    "\n",
    "</font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> Our first step is to <b>import them:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyproj # this can be removed when pyproj is installed on jupyterHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for chdir, getcwd, path.exists\n",
    "import glob # for glob\n",
    "import re # for match\n",
    "import json # for loads\n",
    "import math # for ceil\n",
    "import datetime # for date\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from pyproj import Proj, transform\n",
    "import pandas as pd # for DatetimeIndex\n",
    "import gdal # for gdalbuildvr, gdalmerge, gdalwarp, gdal_translate, Open\n",
    "from osgeo import gdal # for Info\n",
    "import numpy as np # for copy, isnan, log10, ma.masked_where, max, mean, min, percentile, power, unique, var, where \n",
    "import matplotlib.pylab as plb # for figure, grid, rcParams, savefig\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import matplotlib.patches as patches  # for Rectangle\n",
    "from matplotlib import rc\n",
    "from asf_hyp3 import API # for get_products, get_subscriptions, login\n",
    "import scipy # for signal.savgol_filter()\n",
    "import scipy.signal\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from asf_notebook import path_exists\n",
    "from asf_notebook import download_hyp3_products\n",
    "from asf_notebook import new_directory\n",
    "from asf_notebook import earthdata_hyp3_login\n",
    "from asf_notebook import remove_nan_filled_tifs\n",
    "from asf_notebook import select_RTC_polarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Setup matplotlib plotting</b> inside the notebook:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 1. Load Your Own Data Stack Into the Notebook </b> </font> \n",
    "\n",
    "<font size=\"3\"> This notebook assumes that you've created your own data stack over your personal area of interest using the <a href=\"https://www.asf.alaska.edu/\" target=\"_blank\">Alaska Satellite Facility's</a> value-added product system <a href=\"http://hyp3.asf.alaska.edu/\" target=\"_blank\">HyP3</a>. HyP3 is an environment that is used by ASF to prototype value added products and provide them to users to collect feedback. \n",
    "\n",
    "This notebook expects <a href=\"https://media.asf.alaska.edu/uploads/RTC/rtc_atbd_v1.2_final.pdf\" target=\"_blank\">Radiometric Terrain Corrected</a> (RTC) image products as input, so be sure to select an RTC process when creating the subscription for your input data within HyP. Prefer a **unique orbit geometry** (ascending or descending) to keep geometric differences between images low. \n",
    "\n",
    "We will retrieve HyP3 data via the HyP3 API. As both HyP3 and the Notebook environment sit in the <a href=\"https://aws.amazon.com/\" target=\"_blank\">Amazon Web Services (AWS)</a> cloud, data transfer is quick and cost effective.</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> To download data from ASF, you need to provide your <a href=\"https://www.asf.alaska.edu/get-data/get-started/free-earthdata-account/\" target=\"_blank\">NASA Earth Data</a> username to the system. Setup an EarthData account if you do not yet have one. <font color='rgba(200,0,0,0.2)'><b>Note that EarthData's End User License Agreement (EULA) applies when accessing the Hyp3 API from this notebook. If you have not acknowleged the EULA in EarthData, you will need to navigate to <a href=\"https://earthdata.nasa.gov/\" target=\"_blank\">EarthData's home page</a> and complete that process.</b></font>\n",
    "<br><br>\n",
    "<b>Login to Earthdata:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = earthdata_hyp3_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Before we download anything, let's <b>first create a working directory for this analysis and change into it:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/jovyan/notebooks/SAR_Training/English/data_time_series_pickle_tile\"\n",
    "new_directory(path)\n",
    "os.chdir(path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a folder in which to download your RTC products.</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_directory(\"rtc_products\")\n",
    "products_path = f\"{path}/rtc_products\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a folder in which to store pickled data:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory(f\"{path}/pickles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Set a date range of products to download:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = [datetime.date(2018, 6, 1), datetime.date(2019, 1, 1)] #enter your date range here\n",
    "direction = 'D' # enter a flight direction here (A or D)\n",
    "#flight_path = 114 # enter a flight path\n",
    "\n",
    "########## NOTE: Currently filtering by path and flight_direction does not work for InSAR products #########\n",
    "\n",
    "# uncomment code below to download all products\n",
    "#date_range = [None, None]\n",
    "#direction = None\n",
    "flight_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Download the products associated with an existing RTC subscription.</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subscription_id = download_hyp3_products(\n",
    "    api, products_path, start_date=date_range[0], end_date=date_range[1], flight_direction=direction, path=flight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"><b>Determine the subscription's process type</b>, which we need in order to determine the file paths to the tiffs.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_info = api.get_subscription(subscription_id)\n",
    "process_type = subscription_info['process_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a variable called <i>paths</i>, that holds the paths to the images</b>, which varies based on process type:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtc_path = \"rtc_products\"\n",
    "paths = select_RTC_polarization(process_type, rtc_path)\n",
    "print(f\"paths: {paths}\")\n",
    "polarization = paths.split('.')[0][-2:]\n",
    "print(polarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to collect the product acquisition dates:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(paths, split_index, date_start_index, date_end_index):\n",
    "    dates = []\n",
    "    pths = glob.glob(paths)\n",
    "    for p in pths:\n",
    "        date = p.split(\"_\")[split_index][date_start_index:date_end_index]\n",
    "        dates.append(date)\n",
    "    dates.sort()\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Call get_dates() to collect the product acquisition dates:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = get_dates(paths, 5, 0, 8)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> You may notice duplicates in your acquisition dates. As HyP3 processes SAR data on a frame-by-frame basis, duplicates may occur if your area of interest is covered by two consecutive  image frames. In this case, two separate images are generated that need to be merged together before time series processing can commence. \n",
    "<br><br>\n",
    "<b>Write functions to collect and print the paths of the tiffs:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_paths(paths):\n",
    "    tiff_paths = !ls $paths | sort -t_ -k5,5\n",
    "    return tiff_paths\n",
    "\n",
    "def print_tiff_paths(tiff_paths):\n",
    "    print(\"Tiff paths:\")\n",
    "    for p in tiff_paths:\n",
    "        print(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Collect and print the paths of the tiffs:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = get_tiff_paths(paths)\n",
    "#print_tiff_paths(tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b>1.2 Fix multiple UTM Zone-related issues</b> <br>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"3\">Fix multiple UTM Zone-related issues should they exist in your data set. If multiple UTM zones are found, the following code cells will identify the predominant UTM zone and reproject the rest into that zone. This step must be completed prior to merging frames or performing any analysis.</font>\n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"><b>Use gdal.Info to determine the UTM definition types and zones in each product:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones = []\n",
    "utm_types = []\n",
    "print('Checking UTM Zones in the data stack ...\\n')\n",
    "for p in tiff_paths:\n",
    "    info = (gdal.Info(p, options = ['-json']))\n",
    "    info = (json.loads(info))['coordinateSystem']['wkt']\n",
    "    zone = info[(len(info)-8):(len(info)-3)]\n",
    "    utm_zones.append(zone)\n",
    "    typ = info[(len(info)-15):(len(info)-11)]\n",
    "    utm_types.append(typ)\n",
    "print(f\"UTM Zones:\\n {utm_zones}\\n\")\n",
    "print(f\"UTM Types:\\n {utm_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Identify the most commonly used UTM Zone in the data:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_info = {}\n",
    "utm_unique, counts = np.unique(utm_zones, return_counts=True)\n",
    "a = np.where(counts == np.max(counts))\n",
    "stack_info.update({'utm': utm_unique[a][0]})\n",
    "print(f\"Predominate UTM Zone: {stack_info['utm']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Reproject images with errant UTMs to the predominate UTM:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject_indicies = [i for i, j in enumerate(utm_zones) if j != stack_info['utm']] #makes list of indicies in utm_zones that need to be reprojected\n",
    "print('--------------------------------------------')\n",
    "print('Reprojecting %4.1f files' %(len(reproject_indicies)))\n",
    "print('--------------------------------------------')\n",
    "for k in reproject_indicies:\n",
    "    temppath = tiff_paths[k].strip()\n",
    "    _, product_name, tiff_name = temppath.split('/')\n",
    "    cmd = f\"gdalwarp -overwrite rtc_products/{product_name}/{tiff_name} rtc_products/{product_name}/r{tiff_name} -s_srs {utm_types[k]}:{utm_zones[k]} -t_srs EPSG:{stack_info['utm']}\"\n",
    "    \n",
    "    #print(f\"Calling the command: {cmd}\")\n",
    "    !{cmd}\n",
    "    rm_command = f\"rm {tiff_paths[k].strip()}\"\n",
    "    #print(f\"Calling the command: {rm_command}\")\n",
    "    !{rm_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = get_tiff_paths(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b>1.3 Merge multiple frames from the same date.</b></font>\n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"><b>Create a set containing each represented date:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = set(dates)\n",
    "print(unique_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Determine which dates have multiple frames. Create a dictionary with each date as a key linked to a value set as an empty string:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dup_dates = defaultdict(int)\n",
    "for date in dates:\n",
    "    dup_dates[date] += 1\n",
    "    \n",
    "print(dup_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Update the key values in dup_paths with the string paths to all the tiffs for each date:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pth in tiff_paths:\n",
    "    date = pth.split('/')[2].split('_')[3][:8]\n",
    "    if dup_dates[date] != 1:\n",
    "        if type(dup_dates[date]) == int:\n",
    "            dup_dates[date] = pth\n",
    "        else:\n",
    "            dup_dates[date] = f\"{dup_dates[date]} {pth}\"\n",
    "    else:\n",
    "        del dup_dates[date]\n",
    "print(dup_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Merge all the frames for each date.</b> Note that this will overwrite the first original tif and delete the remaining.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Performing {len(dup_dates)} merges:\")\n",
    "for i, dup_date in enumerate(dup_dates):\n",
    "    print(f\"\\nMerging {i+1} of {len(dup_dates)}:\\n\")\n",
    "    output = f\"{dup_dates[dup_date].split('/')[0]}/{dup_dates[dup_date].split('/')[1]}/new{dup_dates[dup_date].split('/')[2].split(' ')[0]}\"\n",
    "    gdal_command = f\"gdal_merge.py -o {output} {dup_dates[dup_date]}\"\n",
    "    print(f\"\\n\\nCalling the command: {gdal_command}\\n\")\n",
    "    !{gdal_command}\n",
    "    for pth in dup_dates[dup_date].split(' '):\n",
    "        if pth and path_exists(pth):\n",
    "            os.remove(pth)\n",
    "            print(f\"Deleting: {pth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> <b>Verify that all duplicate dates were resolved:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = get_dates(paths, 5, 0, 8)\n",
    "if len(dates) == len(set(dates)):\n",
    "    print(f\"Duplicate dates resolved.\")\n",
    "else:\n",
    "    print(f\"Duplicate dates still present!\")\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Update the paths of the tiffs:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = get_tiff_paths(paths)\n",
    "#print_tiff_paths(tiff_paths) # uncomment to view paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 2. Create Subset and Stack Up Your Data </b> </font> \n",
    "\n",
    "<font size=\"3\"> Now you are ready to work with your data. The next cells allow you to select an area of interest (AOI; via bounding-box corner coordinates) for your data analysis. Once selected, the AOI is being extracted and a data stack is formed.\n",
    "\n",
    "<b>As a first step, we extract your AOI from the full frames:</b>\n",
    "</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google Maps, get the rough bounding box for the subset\n",
    "# Enter your corner coordinates below\n",
    "#upper_left_x = 89.6921790\n",
    "#upper_left_y = 25.1467490\n",
    "#lower_right_x = 90.267578\n",
    "#lower_right_y = 24.625406\n",
    "\n",
    "#print(f\"upper left x coord: {upper_left_x}\\nupper left y coord: {upper_left_y}\\nlower right x coord: {lower_right_x}\\nlower right y coord: {lower_right_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max extents for 26 bands with 16gb memory\n",
    "#upper_left_x = 89.6921790\n",
    "#upper_left_y = 25.1467490\n",
    "#lower_right_x = 91.05\n",
    "#lower_right_y = 24.215415\n",
    "\n",
    "#upper_left_x = 89.446831\n",
    "#upper_left_y = 25.225842\n",
    "#lower_right_x = 91.413732\n",
    "#lower_right_y = 24.316126\n",
    "\n",
    "upper_left_x = 29.371429\n",
    "upper_left_y = 31.664799\n",
    "lower_right_x = 32.402310\n",
    "lower_right_y = 29.735274\n",
    "\n",
    "\n",
    "print(f\"upper left x coord: {upper_left_x}\\nupper left y coord: {upper_left_y}\\nlower right x coord: {lower_right_x}\\nlower right y coord: {lower_right_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"> <b>Convert the EPSG:4326 coords from Google Maps to the predominate EPSG in the data stack:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_proj = Proj(init=\"epsg:4326\")\n",
    "out_proj = Proj(init=f\"epsg:{stack_info['utm']}\")\n",
    "stack_info.update({'coords': [[None, None], [None, None]]})\n",
    "stack_info['coords'][0][0], stack_info['coords'][0][1] = transform(in_proj, out_proj, \n",
    "                                                                   upper_left_x, upper_left_y)\n",
    "stack_info['coords'][1][0], stack_info['coords'][1][1] = transform(in_proj, out_proj, \n",
    "                                                                   lower_right_x, lower_right_y)\n",
    "print(stack_info['coords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Adjust the bounding box so as to avoid intersecting any pixels</b>, which then requires resampling and may slightly skew the data:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, coords in enumerate(stack_info['coords']):\n",
    "    for x, c in enumerate(coords):\n",
    "        stack_info['coords'][i][x] = round(stack_info['coords'][i][x]/30)*30\n",
    "print(stack_info['coords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> <b>Update the list of all the absolute paths of the tiffs:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = get_tiff_paths(paths)\n",
    "#print_tiff_paths(tiff_paths) # uncomment to view paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>Subset the tiffs and move them from the individual product directories into their own directory, /tiffs:</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory('tiffs')\n",
    "if path_exists('tiffs'):\n",
    "    print(f\"Subsetting {len(tiff_paths)} geotiffs:\")\n",
    "    for i, tiff_path in enumerate(tiff_paths):\n",
    "        print(f\"\\nSubsetting {i+1} of {len(tiff_paths)} geotiffs:\\n\")\n",
    "        _, granule_name, tiff_name = tiff_path.split('/')\n",
    "        g1, g2, g3, date, g4, g5, g6 = tiff_name.split('_')\n",
    "        gdal_command = f\"gdal_translate -projwin {stack_info['coords'][0][0]} {stack_info['coords'][0][1]} {stack_info['coords'][1][0]} {stack_info['coords'][1][1]} -projwin_srs 'EPSG:{stack_info['utm']}' -co \\\"COMPRESS=DEFLATE\\\" -a_nodata 0 {tiff_path} tiffs/{date}_{polarization}.tiff\"\n",
    "        print(f\"Calling the command: {gdal_command}\")\n",
    "        !{gdal_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>Grab the updated paths of the images:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_path = f\"{os.getcwd()}/tiffs/\"\n",
    "tiff_paths = get_tiff_paths(t_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>Delete any subset tifs that are filled with NaNs and contain no data.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_nan_filled_tifs(t_path, tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>Update the list of dates and tiff_paths after removing NaN filled images:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tiff_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = f\"{path}/tiffs\"\n",
    "tiff_paths = get_tiff_paths(tp)\n",
    "subset_tif_paths = \"tiffs/*.tiff\"\n",
    "dates = get_dates(subset_tif_paths, 0, 6, 14)\n",
    "#print(dates) # uncomment to see dates\n",
    "#print_tiff_paths(tiff_paths) # uncomment to see tiff_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Now we stack up the data by creating a virtual raster table with links to all subset data files: </font>\n",
    "<br><br>\n",
    "<font size=\"3\"><b>Create the virtual raster table for the subset GeoTiffs:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalbuildvrt -separate raster_stack.vrt tiffs/*.tiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 3. Now You Can Work With Your Data </b> </font> \n",
    "\n",
    "<font size=\"3\"> Now you are ready to perform time series analysis on your data stack\n",
    "</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.1 Define Data Directory and Path to VRT </b> </font> \n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"><b>Create a variable containing the VRT filename:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"raster_stack.vrt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create an index of timedelta64 data with Pandas:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_info.update({'time_index': pd.DatetimeIndex(dates)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Print the bands and dates for all images in the virtual raster table (VRT):</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 1\n",
    "print(f\"Bands and dates for {image_file}\")\n",
    "for i in stack_info['time_index']:\n",
    "    print(\"{:4d} {}\".format(j, i.date()), end=' ')\n",
    "    j += 1\n",
    "    if j%5 == 1:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.2 Open Your Data Stack and Visualize Some Layers </b> </font> \n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> We will <b>open your VRT</b> and visualize some layers using Matplotlib. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = gdal.Open(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Print the bands, pixels, and lines:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_info.update({'bands': img.RasterCount,\n",
    "                  'pixels': img.RasterXSize,\n",
    "                  'lines': img.RasterYSize})\n",
    "for info in stack_info:\n",
    "    print(f\"{info}: {stack_info[info]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Determine the total number of pixels (bands * pixels * lines) in the VRT:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = stack_info['bands'] * stack_info['pixels'] * stack_info['lines']\n",
    "print(stack_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>If the number of pixels in the VRT is over the limit, the VRT must be tiled and processed in batches to avoid running out of memory:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, sqrt\n",
    "tiling = False\n",
    "limit = 414000000\n",
    "if stack_size > limit:\n",
    "    tiling = True\n",
    "    tile_count = ceil(stack_size / limit)\n",
    "    tile_1D = ceil(sqrt(tile_count))\n",
    "    print(tile_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tiling:\n",
    "    tile_size_pixels = ceil(stack_info['pixels'] / tile_1D)\n",
    "    tile_size_lines = ceil(stack_info['lines'] / tile_1D)\n",
    "\n",
    "    print(f\"Pixels per tile: {tile_size_pixels}\")\n",
    "    print(f\"Lines per tile: {tile_size_lines}\")\n",
    "    print(f\"Tile Size (including all bands): {tile_size_pixels * tile_size_lines * stack_info['bands']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_dir = 'tiles'\n",
    "if tiling:\n",
    "    new_directory(tile_dir)\n",
    "    os.chdir(tile_dir)\n",
    "    for i in range(0, stack_info['pixels'], tile_size_pixels):\n",
    "        for j in range(0, stack_info['lines'], tile_size_lines):\n",
    "            cmd = f\"gdal_translate -of VRT -srcwin {str(i)}, {str(j)}, {str(tile_size_pixels)}, {str(tile_size_lines)} {path}/{image_file} tile_{str(i)}_{str(j)}.vrt\"\n",
    "            !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_paths(tile_dir):\n",
    "    tile_paths = glob.glob('tile*.vrt')\n",
    "    tile_paths.sort()\n",
    "    return tile_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_paths = get_tile_paths(f\"{path}/{tile_dir}\")\n",
    "print(tile_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.3 Calibration and Data Conversion between dB and Power Scales </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> <font color='rgba(200,0,0,0.2)'> <b>Note, that if your data were generated by HyP3, this step is not necessary!</b> HyP3 performs the full data calibration and provides you with calibrated data in power scale. </font>\n",
    "    \n",
    "If, your data is from a different source, however, calibration may be necessary to ensure that image gray values correspond to proper radar cross section information. \n",
    "\n",
    "Calibration coefficients for SAR data are often defined in the decibel (dB) scale due to the high dynamic range of the imaging system. For the L-band ALOS PALSAR data at hand, the conversion from uncalibrated DN values to calibrated radar cross section values in dB scale is performed by applying a standard **calibration factor of -83 dB**. \n",
    "<br> <br>\n",
    "$\\gamma^0_{dB} = 20 \\cdot log10(DN) -83$\n",
    "\n",
    "The data at hand are radiometrically terrain corrected images, which are often expressed as terrain flattened $\\gamma^0$ backscattering coefficients. For forest and land cover monitoring applications $\\gamma^o$ is the preferred metric.\n",
    "\n",
    "<b>To apply the calibration constant for your data and export in *dB* scale, uncomment the following code cell</b>: </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #caldB=20*np.log10(raster_stack)-83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"> While **dB**-scaled images are often \"visually pleasing\", they are often not a good basis for mathematical operations on data. For instance, when we compute the mean of observations, it makes a difference whether we do that in power or dB scale. Since dB scale is a logarithmic scale, we cannot simply average data in that scale. \n",
    "    \n",
    "Please note that the **correct scale** in which operations need to be performed **is the power scale.** This is critical, e.g. when speckle filters are applied, spatial operations like block averaging are performed, or time series are analyzed.\n",
    "\n",
    "To **convert from dB to power**, apply: $\\gamma^o_{pwr} = 10^{\\frac{\\gamma^o_{dB}}{10}}$ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calPwr=np.power(10.,caldB/10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.4 Create a Time Series Animation </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\">Now we are ready to create a time series animation from the calibrated SAR data.\n",
    "<br><br>\n",
    "<b>Write a function to mask a raster stack:</b>\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_raster_stack(raster_stack):\n",
    "    return np.ma.masked_where(raster_stack==0, raster_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to create and save a time series animation. Returns a raster stack:</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series(gdal_img, output_filename):\n",
    "    band = gdal_img.GetRasterBand(1)\n",
    "    raster0 = band.ReadAsArray()\n",
    "    band_number = 0 # Needed for updates\n",
    "    rastr_stack = gdal_img.ReadAsArray()\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    ax = fig.subplots()\n",
    "    ax.axis('off')\n",
    "    vmin = np.percentile(rastr_stack.flatten(), 5)\n",
    "    vmax = np.percentile(rastr_stack.flatten(), 95)\n",
    "    \n",
    "    r0dB = 20 * np.log10(raster0) - 83\n",
    "\n",
    "    im = ax.imshow(raster0, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(\"{}\".format(stack_info['time_index'][0].date()))\n",
    "\n",
    "    def animate(i):\n",
    "        ax.set_title(\"{}\".format(stack_info['time_index'][i].date()))\n",
    "        im.set_data(rastr_stack[i])\n",
    "\n",
    "    # Interval is given in milliseconds\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=rastr_stack.shape[0], interval=400)\n",
    "    \n",
    "    rc('animation', embed_limit=40971520.0)  # We need to increase the limit maybe to show the entire animation\n",
    "    \n",
    "    HTML(ani.to_jshtml())\n",
    "    \n",
    "    ani.save(output_filename, writer='pillow', fps=2)\n",
    "    return rastr_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create and move into a directory in which to store our plots and animations:</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "product_path = 'plots_and_animations'\n",
    "new_directory(product_path)\n",
    "if path_exists(product_path) and os.getcwd() != f\"{path}/{product_path}\":\n",
    "    os.chdir(product_path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Delete the dummy png</b> that was saved to the current working directory while generating the javascript animation in the last code cell.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "try:\n",
    "    os.remove('None0000000.png')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.5 Plot the Time Series of Means Calculated Across the Subset </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> To create the time series of means, we will go through the following steps:\n",
    "1. Ensure that you use the data in **power scale** ($\\gamma^o_{pwr}$) for your mean calculations.\n",
    "2. compute means.\n",
    "3. convert the resulting mean values into dB scale for visualization.\n",
    "4. plot time series of means. </font> \n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"> <b>Compute the means:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_means(raster_stack_masked, output_filename):\n",
    "    rs_means_pwr = np.mean(raster_stack_masked, axis=(1, 2))\n",
    "    \n",
    "    rs_means_dB = 10.*np.log10(rs_means_pwr)\n",
    "    \n",
    "    \n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    fig = plt.figure(figsize=(16, 4))\n",
    "    ax1 = fig.subplots()\n",
    "    window_length = len(rs_means_pwr)-1\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    polyorder = math.ceil(window_length*0.1)\n",
    "    yhat = scipy.signal.savgol_filter(rs_means_pwr, window_length, polyorder) \n",
    "    ax1.plot(stack_info['time_index'], \n",
    "             yhat, color='red', \n",
    "             marker='o', \n",
    "             markerfacecolor='white', \n",
    "             linewidth=3, markersize=6)\n",
    "    ax1.plot(stack_info['time_index'], \n",
    "             rs_means_pwr, \n",
    "             color='gray', \n",
    "             linewidth=0.5)\n",
    "    plt.grid()\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('$\\overline{\\gamma^o}$ [power]')\n",
    "    plt.savefig(output_filename, dpi=72, transparent='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rs_means_pwr = np.mean(raster_stack_masked, axis=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Convert resulting mean value time-series to dB scale for visualization:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rs_means_dB = 10.*np.log10(rs_means_pwr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Plot and save the time series of means (RCSoverTime.png):</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.6 Calculate Coefficient of Variance </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> The coefficient of variance describes how much the $\\sigma_{0}$ or $\\gamma_{0}$ measurements in a pixel vary over time. Hence, the coefficient of variance can indicate different vegetation cover and soil moisture regimes in your area.</font> \n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to convert our plots into GeoTiffs:</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geotiff_from_plot(source_image, out_filename, extent, utm, cmap=None, vmin=None, vmax=None, interpolation=None, dpi=300):\n",
    "    assert \".\" not in out_filename, 'Error: Do not include the file extension in out_filename'\n",
    "    assert type(extent) == list and len(extent) == 2 and len(extent[0]) == 2 and len(\n",
    "        extent[1]) == 2, 'Error: extent must be a list in the form [[upper_left_x, upper_left_y], [lower_right_x, lower_right_y]]'\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(source_image, cmap=cmap, vmin=vmin, vmax=vmax, interpolation=interpolation)\n",
    "    temp = f\"{out_filename}_temp.png\"\n",
    "    plt.savefig(temp, dpi=dpi, transparent='true', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    cmd = f\"gdal_translate -of Gtiff -a_ullr {extent[0][0]} {extent[0][1]} {extent[1][0]} {extent[1][1]} -a_srs EPSG:{utm} {temp} {out_filename}.tiff\"\n",
    "    !{cmd}\n",
    "    try:\n",
    "        os.remove(temp)\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to plot the Coefficient of Variance Map and save it as a png (Coeffvar.png):</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coeff_var_map(raster_stck, output_filename):\n",
    "    test = np.var(raster_stck,0)\n",
    "    mtest = np.mean(raster_stck[raster_stck.nonzero()],0)\n",
    "    coeffvar = test/(mtest+0.001)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    fig = plt.figure(figsize=(13, 10))\n",
    "    ax = fig.subplots()\n",
    "    ax.axis('off')\n",
    "    vmin = np.percentile(coeffvar.flatten(), 5)\n",
    "    vmax = np.percentile(coeffvar.flatten(), 95)\n",
    "    ax.set_title('Coefficient of Variance Map')\n",
    "\n",
    "    im = ax.imshow(coeffvar, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.savefig(output_filename, dpi=300, transparent='true')\n",
    "    return {'coeffvar': coeffvar, 'vmin': vmin, 'vmax': vmax}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.7 Threshold Coefficient of Variance Map </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> This is an example how to threshold the derived coefficient of variance map. This can be useful, e.g., to detect areas of active agriculture.</font> \n",
    "<br><br>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to plot and save the coefficient of variance histogram and CDF (thresh_coeff_var_histogram.png):</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresh_coeff_var_histogram(coeffvar, output_filename):\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    fig = plt.figure(figsize=(14, 6)) # Initialize figure with a size\n",
    "    ax1 = fig.add_subplot(121)  # 121 determines: 2 rows, 2 plots, first plot\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    # Second plot: Histogram\n",
    "    # IMPORTANT: To get a histogram, we first need to *flatten* \n",
    "    # the two-dimensional image into a one-dimensional vector.\n",
    "    h = ax1.hist(coeffvar.flatten(), bins=200, range=(0, 0.03))\n",
    "    ax1.xaxis.set_label_text('Coefficient of Variation')\n",
    "    ax1.set_title('Coeffvar Histogram')\n",
    "    plt.grid()\n",
    "    n, bins, patches = ax2.hist(coeffvar.flatten(), bins=200, range=(0, 0.03), cumulative='True', density='True', histtype='step', label='Empirical')\n",
    "    ax2.xaxis.set_label_text('Coefficient of Variation')\n",
    "    ax2.set_title('Coeffvar CDF')\n",
    "    plt.grid()\n",
    "    plt.savefig(output_filename, dpi=72, transparent='true')\n",
    "    return {'n': n, 'bins': bins}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to plot the Threshold Coefficient of Variance Map and save it as a png (Coeffvarthresh.png):</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresh_coeff_var_map(coeffvar, output_filename, n_bins):\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    outind = np.where(n_bins['n'] > 0.85)\n",
    "    threshind = np.min(outind)\n",
    "    thresh = n_bins['bins'][threshind]\n",
    "    coeffvarthresh = np.copy(coeffvar)\n",
    "    coeffvarthresh[coeffvarthresh < thresh] = 0\n",
    "    coeffvarthresh[coeffvarthresh > 0.1] = 0\n",
    "    fig = plt.figure(figsize=(13, 10))\n",
    "    ax = fig.subplots()\n",
    "    ax.axis('off')\n",
    "    vmin = np.percentile(coeffvar.flatten(), 5)\n",
    "    vmax = np.percentile(coeffvar.flatten(), 95)\n",
    "    ax.set_title(r'Thresholded Coeffvar Map [$\\alpha=95%$]')\n",
    "    im = ax.imshow(coeffvarthresh, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    bar = fig.colorbar(im, ax=ax)\n",
    "    plt.savefig(output_filename, dpi=300, transparent='true')\n",
    "    return {'coeffvarthresh': coeffvarthresh, 'vmin': vmin, 'vmax': vmax}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to create a stack_info dictionary for a tile:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_stack_info(stack_info):\n",
    "    tile_stack_info = deepcopy(stack_info)\n",
    "    tile_info = (gdal.Info(img, options = ['-json']))\n",
    "    tile_corner_coords = (json.loads(tile_info))['cornerCoordinates']\n",
    "    \n",
    "    tile_stack_info['coords'][0][0] = tile_corner_coords['upperLeft'][0]\n",
    "    tile_stack_info['coords'][0][1] = tile_corner_coords['upperLeft'][1]\n",
    "    tile_stack_info['coords'][1][0] = tile_corner_coords['lowerRight'][0]\n",
    "    tile_stack_info['coords'][1][1] = tile_corner_coords['lowerRight'][1]\n",
    "   \n",
    "    tile_stack_info.update({'bands': img.RasterCount,\n",
    "                  'pixels': img.RasterXSize,\n",
    "                  'lines': img.RasterYSize})\n",
    "    \n",
    "    return tile_stack_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to pickle data:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ferment(cucumber, output_filename):\n",
    "    pf = open(output_filename, 'ab')\n",
    "    pickle.dump(cucumber, pf)\n",
    "    pf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "for tile in tile_paths:\n",
    "    img = gdal.Open(f\"{path}/{tile_dir}/{tile}\")\n",
    "    tile_id = f\"{tile.split('_')[1]}_{tile.split('_')[2].split('.')[0]}\"\n",
    "    \n",
    "    tile_stack_info = get_tile_stack_info(stack_info)\n",
    "    \n",
    "    print(f\"Tile: {tile_id}\")\n",
    "    #print(f\"stack_info coords: {stack_info['coords']}\")\n",
    "    #print(tile_stack_info['coords'])\n",
    "          \n",
    "    ferment(tile_stack_info, f\"{path}/pickles/pickled_tile_stack_info_{tile_id}\")\n",
    "    \n",
    "    raster_stack = create_time_series(img, f\"animation_{tile_id}.gif\")\n",
    "    plt.close('all')\n",
    "    gc.collect()\n",
    "    ferment(raster_stack, f\"{path}/pickles/pickled_tile_raster_stack_{tile_id}\")\n",
    "           \n",
    "    raster_stack_masked = mask_raster_stack(raster_stack)\n",
    "    create_time_series_means(raster_stack_masked, f\"RCSoverTime_{tile_id}.png\")\n",
    "    plt.close('all')\n",
    "    gc.collect()\n",
    "    ferment(raster_stack_masked, f\"{path}/pickles/pickled_masked_tile_raster_stack_{tile_id}\")\n",
    "    \n",
    "    coeffvar_data = create_coeff_var_map(raster_stack, f\"Coeffvar_{tile_id}.png\")\n",
    "    plt.close('all')\n",
    "    gc.collect()\n",
    "    geotiff_from_plot(coeffvar_data['coeffvar'], f\"Coeffvar_{tile_id}\", tile_stack_info['coords'], tile_stack_info['utm'], cmap='jet', vmin=coeffvar_data['vmin'], vmax=coeffvar_data['vmax'])\n",
    "    \n",
    "    n_b = create_thresh_coeff_var_histogram(coeffvar_data['coeffvar'], f\"thresh_coeff_var_histogram_{tile_id}.png\")\n",
    "    \n",
    "    coeffvarthresh_data = create_thresh_coeff_var_map(coeffvar_data['coeffvar'], f\"Coeffvarthresh_{tile_id}.png\", n_b)\n",
    "    plt.close('all')\n",
    "    gc.collect()\n",
    "    geotiff_from_plot(coeffvarthresh_data['coeffvarthresh'], 'Coeffvarthresh', tile_stack_info['coords'], tile_stack_info['utm'], cmap='jet', vmin=coeffvarthresh_data['vmin'], vmax=coeffvarthresh_data['vmax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657 Microwave Remote Sensing - Version 1.0 - March 2019 </i>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
